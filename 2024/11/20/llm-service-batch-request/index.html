

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/logo.jpg">
  <link rel="icon" href="/logo.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="habaneraa">
  <meta name="keywords" content="">
  
    <meta name="description" content="引言假设我们有一项需要 LLM 完成的任务和一批数据，希望通过调用 API 的方式获得 LLM 输出来处理这批任务。比如，以分类任务为例，对每一个样本，需要根据提示词模版构建提示词，然后发送一个请求给 OpenAI 等 LLM 服务提供方，再解析输出得到分类结果。 这个过程看似非常简单直观，但在实践中可能会面对诸多问题，尤其是在数量较大时（比如 10 万以上样本量，全部请求总量超过 10 mill">
<meta property="og:type" content="article">
<meta property="og:title" content="大批量调用 LLM 服务实战：以分类任务为例">
<meta property="og:url" content="https://habaneraa.github.io/2024/11/20/llm-service-batch-request/index.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="引言假设我们有一项需要 LLM 完成的任务和一批数据，希望通过调用 API 的方式获得 LLM 输出来处理这批任务。比如，以分类任务为例，对每一个样本，需要根据提示词模版构建提示词，然后发送一个请求给 OpenAI 等 LLM 服务提供方，再解析输出得到分类结果。 这个过程看似非常简单直观，但在实践中可能会面对诸多问题，尤其是在数量较大时（比如 10 万以上样本量，全部请求总量超过 10 mill">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-11-20T05:03:37.000Z">
<meta property="article:modified_time" content="2024-11-20T05:03:37.000Z">
<meta property="article:author" content="habaneraa">
<meta property="article:tag" content="python">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>大批量调用 LLM 服务实战：以分类任务为例 - 博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/fonts.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"habaneraa.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":["home"]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 45vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/blue_umbrella.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">大批量调用 LLM 服务实战：以分类任务为例</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-11-20 13:03" pubdate>
          2024年11月20日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.2k 字
        
      </span>
    

    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">大批量调用 LLM 服务实战：以分类任务为例</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>假设我们有一项需要 LLM 完成的任务和一批数据，希望通过调用 API 的方式获得 LLM 输出来处理这批任务。比如，以分类任务为例，对每一个样本，需要根据提示词模版构建提示词，然后发送一个请求给 OpenAI 等 LLM 服务提供方，再解析输出得到分类结果。</p>
<p>这个过程看似非常简单直观，但在实践中可能会面对诸多问题，尤其是在数量较大时（比如 10 万以上样本量，全部请求总量超过 10 million tokens）…</p>
<ol>
<li><strong>如何获得百分百可靠的输出结果？</strong> 任务需要解析生成的文本，得到分类标签。LLM 不一定总会遵循指令，而且可能还要考虑 COT 生成、模型拒答等情况，纯文本解析显得不够可靠。</li>
<li><strong>如何实现并发？</strong> 对于单个生成任务，LLM 的响应速度是很难提升的，所以几万样本顺序执行的时间开销可能会非常大。我们知道，无论是 OpenAI 等 API 服务商还是本地部署的 LLM 推理，都支持并发处理，且效率会高很多。这时我们就需要在客户端实现优雅的并发请求逻辑。</li>
<li><strong>如何缓存调用结果避免浪费 API 费用？</strong> 大批量调用 API 会产生不菲的开销。我们可能会中断任务处理问题、调整实现，但我们不希望已经请求过的内容“浪费掉”，所以把每次请求的结果进行缓存，不仅能提高重复执行时的速度，还能避免浪费 token 费用。</li>
<li><strong>如何处理各类异常？</strong> 脚本执行过程中可能会产生各种错误，从最常见的网络错误，到请求构建和结果解析，都有可能抛出异常。我们不希望批量执行被任何错误中断，所以需要充分考虑可能会出现的异常类型并加以处理。</li>
</ol>
<p>针对以上问题，本文讨论一些简单有效的解决方案，并附上 Python 实现。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们使用 LangChain 来简化一些功能的实现。我们的任务只涉及单次 LLM 调用，所以手工实现相关逻辑也是不难的，用 LangChain 仅仅是起到缩减代码量的效果。当然如果有更复杂的任务逻辑，相信 LangChain 也能帮到很多。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install langchain langchain-openai langchain-community<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain_openai <span class="hljs-keyword">import</span> ChatOpenAI<br><span class="hljs-keyword">from</span> langchain_core.prompts <span class="hljs-keyword">import</span> ChatPromptTemplate<br><span class="hljs-keyword">from</span> langchain_core.runnables <span class="hljs-keyword">import</span> RunnableLambda<br><span class="hljs-keyword">from</span> langchain_core.messages <span class="hljs-keyword">import</span> AIMessage<br><span class="hljs-keyword">from</span> langchain_community.cache <span class="hljs-keyword">import</span> SQLiteCache<br></code></pre></td></tr></table></figure>

<p>我们假设数据是 JSON Lines 格式，每一行分别代表一个样本，每个样本都由固定格式的 JSON 对象描述，非常灵活易用。同时，相比于 JSON，每个文本行一个样本也可以避免一次读入全部文件到内存，IO 占用时间。</p>
<p>大模型服务方面，我们采用最典型的 OpenAI API 格式，因为许多国内厂商都会与之兼容。</p>
<h2 id="结构化输出"><a href="#结构化输出" class="headerlink" title="结构化输出"></a>结构化输出</h2><p>首先，引导大模型生成结构化输出能让我们的任务更加可靠，具体来说就是让 LLM 输出 JSON 文本，然后再解析出 JSON 对象的字段得到答案。</p>
<p>今年八月，OpenAI 发布了 structured outputs 特性，允许开发者指定输出格式，模型会百分比可靠地按 JSON schema 输出结果 <sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://openai.com/index/introducing-structured-outputs-in-the-api/">[1]</span></a></sup>。不过考虑到不是所有平台都支持 json schema，我们这次就只使用普通的 JSON 输出。首先，在提示词部分提供指令和示例输出：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-section">## Response Format</span><br><br>Your response should be in JSON format, containing fields: <span class="hljs-code">`reasoning_steps`</span>, steps that analyze provided content and lead to a classification conclusion, and <span class="hljs-code">`category`</span>, must be one of the candidate categories. The steps should be concise and accurate as much as you can. Here is an example of the desired format:<br><br>&#123;&#123;<br>  &quot;reasoning<span class="hljs-emphasis">_steps&quot;: [</span><br><span class="hljs-emphasis">    &quot;first...&quot;,</span><br><span class="hljs-emphasis">    &quot;then...&quot;</span><br><span class="hljs-emphasis">  ],</span><br><span class="hljs-emphasis">  &quot;category&quot;: <span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">label</span>&gt;</span></span></span><br><span class="hljs-emphasis">&#125;&#125;</span><br></code></pre></td></tr></table></figure>

<p>我们引导模型先生成一系列 “推理步骤”，然后再生成标签字段 “category” ，这是利用了思维链技巧提高准确性。</p>
<p>使用 LangChain 创建一个 LLM 抽象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">llm = ChatOpenAI(<br>	model=<span class="hljs-string">&quot;...&quot;</span>,<br>	api_key=<span class="hljs-string">&quot;...&quot;</span>,<br>	base_url=<span class="hljs-string">&quot;...&quot;</span>,<br>	temperature=<span class="hljs-number">0.0</span>,<br>	top_p=<span class="hljs-number">0.001</span>,<br>).bind(response_format=&#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;json_object&quot;</span>&#125;)<br></code></pre></td></tr></table></figure>

<p>创建提示词模版对象。我们把提示词放在一个本地文件方便编辑，注意文本中的 <code>&#123;field&#125;</code> 是可被替换的模版占位符，要想使用真正的花括号需要使用两个花括号来转义。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path<br>prompt = Path(<span class="hljs-string">&quot;./prompt.md&quot;</span>).read_text()<br>prompt_template = ChatPromptTemplate.from_messages([<br>	(<span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;You are a helpful assistant.&quot;</span>),<br>	(<span class="hljs-string">&quot;user&quot;</span>, prompt),<br>])<br></code></pre></td></tr></table></figure>

<p>然后我们需要编写处理输出的逻辑。LangChain 对象 <code>ChatOpenAI</code> 调用 <code>invoke()</code> 会返回一个 <code>AIMessage</code> 对象，最简单的处理方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_response</span>(<span class="hljs-params">response: AIMessage</span>) -&gt; <span class="hljs-built_in">dict</span>:<br>    parsed = json.loads(response.content)<br>    <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;category&quot;</span>: parsed[<span class="hljs-string">&quot;category&quot;</span>]&#125;<br><br>chain = prompt_template | llm | RunnableLambda(parse_response)<br></code></pre></td></tr></table></figure>

<p>以上代码创建了一个 chain，它会把输入传给提示词模版，生成提示词后调用 LLM，然后再调用我们自定义的 <code>parse_response</code> 方法，解析 JSON 字符串，最终返回一个字典代表输出。</p>
<p>进一步地，我们还可以获取 LLM 对该类别的概率（虽然这有别于机器学习中的类别概率），这可以通过 logprob 字段实现 <sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://python.langchain.com/docs/how_to/logprobs/">[2]</span></a></sup>。由于大模型会按格式输出 <code>&quot;category&quot;: &quot;&lt;label&gt;&quot;</code> ，所以我们需要借助 <code>&quot;category&quot;: &quot;</code> 前缀来找到 <code>&lt;label&gt;</code> 序列中的的第一个 token，实现方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">llm = ChatOpenAI(<br>    ...<br>    logprobs=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">find_target_token</span>(<span class="hljs-params">token_list: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">dict</span>]</span>):<br>	pattern = [<span class="hljs-string">&#x27; &quot;&#x27;</span>, <span class="hljs-string">&#x27;category&#x27;</span>, <span class="hljs-string">&#x27;&quot;:&#x27;</span>, <span class="hljs-string">&#x27; &quot;&#x27;</span>]<br>	pattern_length = <span class="hljs-built_in">len</span>(pattern)<br>	<span class="hljs-comment"># Loop through the token list with a sliding window</span><br>	<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(token_list) - pattern_length):<br>		current_tokens = [token_dict[<span class="hljs-string">&#x27;token&#x27;</span>] <span class="hljs-keyword">for</span> token_dict <span class="hljs-keyword">in</span> token_list[i:i + pattern_length]]<br>		<span class="hljs-keyword">if</span> current_tokens == pattern:<br>			<span class="hljs-keyword">if</span> i + pattern_length &lt; <span class="hljs-built_in">len</span>(token_list):<br>				<span class="hljs-keyword">return</span> token_list[i + pattern_length]<br>	<span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_response</span>(<span class="hljs-params">response: AIMessage</span>) -&gt; <span class="hljs-built_in">dict</span>:<br>	res = find_target_token(response.response_metadata[<span class="hljs-string">&quot;logprobs&quot;</span>][<span class="hljs-string">&quot;content&quot;</span>])<br>	answer_logprob = <span class="hljs-built_in">float</span>(res[<span class="hljs-string">&quot;logprob&quot;</span>]) <span class="hljs-keyword">if</span> res <span class="hljs-keyword">else</span> -<span class="hljs-number">0.0</span><br>	answer_prob = math.exp(answer_logprob)<br>	...<br></code></pre></td></tr></table></figure>

<p>这样，我们就实现了解析 JSON 结构化输出，获取类别标签 + 类别概率。</p>
<h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><p>首先要考虑的是输出解析部分的异常情况。当输出解析部分出现任何问题时，我们记录该问题，然后返回解析前的原始消息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_response</span>(<span class="hljs-params">response: AIMessage</span>) -&gt; <span class="hljs-built_in">dict</span>:<br>	<span class="hljs-keyword">try</span>:<br>		res = find_target_token(response.response_metadata[<span class="hljs-string">&quot;logprobs&quot;</span>][<span class="hljs-string">&quot;content&quot;</span>])<br>		answer_logprob = <span class="hljs-built_in">float</span>(res[<span class="hljs-string">&quot;logprob&quot;</span>]) <span class="hljs-keyword">if</span> res <span class="hljs-keyword">else</span> -<span class="hljs-number">0.0</span><br>		answer_prob = math.exp(answer_logprob)<br>		parsed = json.loads(response.content)<br>		steps = <span class="hljs-string">&quot;\n&quot;</span>.join(parsed[<span class="hljs-string">&quot;reasoning_steps&quot;</span>])<br>		answer = <span class="hljs-built_in">str</span>(parsed[<span class="hljs-string">&quot;category&quot;</span>])<br>		<span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;category:&quot;</span>: answer, <span class="hljs-string">&quot;prob&quot;</span>: answer_prob, <span class="hljs-string">&quot;steps&quot;</span>: steps&#125;<br>	<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>		logger.warning(<span class="hljs-string">f&quot;LLM response parsing error: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br>		<span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;category:&quot;</span>: <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;raw&quot;</span>: response.content&#125;<br></code></pre></td></tr></table></figure>

<p>这样，程序不会因为模型输出部分的问题而中断，后续也可以追溯这些意外情况。</p>
<p>对于 API 相关的异常，例如网络问题、rate limit 问题等，实际上 LangChain 提供了相关逻辑，它默认会在网络问题发生时进行重试，重试次数可以通过 <code>BaseChatOpenAI.max_retries</code> 参数控制 <sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://python.langchain.com/docs/concepts/chat_models/#standard-parameters">[3]</span></a></sup>。而速率限制处理行为可以通过指定 <code>rate_limiter</code> 来控制 <sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://python.langchain.com/docs/how_to/chat_model_rate_limiting/">[4]</span></a></sup>。</p>
<p>在处理异常情况时，我们难免会重新运行，当数据量大时，调用 LLM 的费用就显得浪费不起了。我们可以在 API 调用处增加一层缓存，这样我们的批量任务可以随时中断调试，不用担心任何浪费。所幸的是 LangChain 已经提供了相关功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">llm_response_cache_dir = Path(<span class="hljs-string">&quot;.langchain.db&quot;</span>).as_posix()<br>llm_cache = SQLiteCache(database_path=llm_response_cache_dir)<br>llm = ChatOpenAI(<br>    ...<br>    cache=llm_cache,<br>)<br></code></pre></td></tr></table></figure>

<p>这段 <code>llm_cache</code> 做的事情实际上是把每一次面向 LLM API 的请求内容和结果都做键值对缓存，当发现完全相同的请求时，直接返回结果，不走网络 API，从而实现节省时间和费用的效果。缓存使用一个本地 sqlite 文件实现。</p>
<p>最后，当真正执行大批量任务，不希望被任何异常中断，我们可以在最上层代码加一层捕获，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">try</span>:<br>    result = chain.invoke(inputs)<br>    <span class="hljs-keyword">return</span> &#123;**result, **inputs&#125;<br><span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>    logger.exception(<span class="hljs-string">f&quot;Error: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;error&quot;</span>: <span class="hljs-built_in">str</span>(e), **inputs&#125;<br></code></pre></td></tr></table></figure>

<h2 id="基于协程的并发请求"><a href="#基于协程的并发请求" class="headerlink" title="基于协程的并发请求"></a>基于协程的并发请求</h2><p>假设已经定义好 <code>chain</code> 对象，我们的任务执行起来是这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">tasks: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">dict</span>]<br>results = <span class="hljs-built_in">list</span>()<br><span class="hljs-keyword">for</span> task <span class="hljs-keyword">in</span> tasks:<br>    result = chain.invoke(task)<br>    results.append(result)<br></code></pre></td></tr></table></figure>

<p>这种串行方式会挨个执行等待服务端生成，而 LLM 的生成速度是很慢的。要解决这个问题，OpenAI 提供了 Batches API，但有些情况下我们希望快速得到结果，或者部署服务不支持 Batch 调用形式，这时我们就非常需要自行实现并发请求。</p>
<p>写并发，Python 协程是一个很好的东西，实现的时候还要考虑几个点：</p>
<ol>
<li>数据量可能会非常大，避免一次性把整个数据集都创建出协程</li>
<li>虽然 LangChain 已经帮我们处理了 rate limits，但并发数需要自己做控制</li>
<li>怎么收集协程结果做进度条显示，毕竟没进度条还是不放心</li>
</ol>
<p>我们一步一步来，首先把 chain 逻辑写成异步函数，使用其异步方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_task</span>(<span class="hljs-params">task: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]</span>) -&gt; <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]:<br>    <span class="hljs-keyword">try</span>:<br>        result = <span class="hljs-keyword">await</span> chain.ainvoke(task)<br>        <span class="hljs-keyword">return</span> &#123;**input_dict, **result,&#125;<br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        logger.exception(<span class="hljs-string">f&quot;Error: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br>        <span class="hljs-keyword">return</span> &#123;**input_dict, <span class="hljs-string">&quot;error&quot;</span>: <span class="hljs-built_in">str</span>(e),&#125;<br></code></pre></td></tr></table></figure>

<p>这个函数的输入直接对应我们的 JSON 输入对象，输出也是字典格式，非常灵活。并且这个异步函数保证不会抛异常（假设我们预料到的异常处理都已经写进 <code>chain</code> 里面了）。</p>
<p>然后，输入部分要分块进行，为此，我们把文件读取写成一个生成器函数，避免过大的内存开销。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_file_in_chunks</span>(<span class="hljs-params">filepath: <span class="hljs-built_in">str</span>, chunk_size: <span class="hljs-built_in">int</span></span>):<br>    current_batch = []<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> file:<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file:<br>            current_batch.append(line.rstrip(<span class="hljs-string">&#x27;\n&#x27;</span>))<br>            <br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(current_batch) &gt;= chunk_size:<br>                <span class="hljs-keyword">yield</span> current_batch<br>                current_batch = []<br>        <br>        <span class="hljs-comment"># Don&#x27;t forget the last partial batch</span><br>        <span class="hljs-keyword">if</span> current_batch:<br>            <span class="hljs-keyword">yield</span> current_batch<br></code></pre></td></tr></table></figure>

<p>调用这个生成器时，我们在循环内部做 JSON 解析转字典格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> read_file_in_chunks(filepath, <span class="hljs-number">1000</span>):<br>    task_dicts = [json.loads(json_text) <span class="hljs-keyword">for</span> json_text <span class="hljs-keyword">in</span> chunk]<br></code></pre></td></tr></table></figure>

<p>这时，对于这一组任务，我们可以创建一批协程，也就是上面定义的 <code>run_task</code>，创建之后使用 <code>gather()</code> 来并发执行协程。例如这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">results = <span class="hljs-keyword">await</span> asyncio.gather(*[run_task(d) <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> task_dicts])<br></code></pre></td></tr></table></figure>

<p>为了控制并发数，很显然这还不够。我们可以使用信号量机制来控制同一时刻能够并发执行的协程数目，为此我们需要在 <code>run_task</code> 外面套一层异步函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">semaphore = asyncio.Semaphore(max_concurrency)<br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">do</span>(<span class="hljs-params">data</span>):<br>	<span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> semaphore:<br>		<span class="hljs-keyword">return</span> <span class="hljs-keyword">await</span> run_task(data)<br></code></pre></td></tr></table></figure>

<p>这里用到了 <code>Semaphore</code> 对象的 context manager 语法，在进出该 with 块时会自动进行获取 &#x2F; 释放操作。在 <code>gather()</code> 调用时，所有 <code>do()</code> 协程都被启动，但只有 <code>max_concurrency</code> 个能够进入 <code>with</code> 块，其他所有都会卡在 <code>with</code> 入口等待信号量释放，如此实现请求并发数的控制。</p>
<p>进一步地，我们还可以在此基础上实现进度显示。我们使用来自 <code>rich</code> 库的 <code>rich.progress.Progress</code> 对象，在终端实时显示进度。我们需要把进度更新放在 <code>run_task()</code> 返回结果的位置上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">progress_bar = progress.Progress()<br><span class="hljs-keyword">with</span> progress_bar:<br>	task_id = progress_bar.add_task()<br>	<br>	<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">do</span>(<span class="hljs-params">data: <span class="hljs-built_in">dict</span></span>) -&gt; <span class="hljs-built_in">dict</span>:<br>		<span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> semaphore:<br>			result = <span class="hljs-keyword">await</span> run_task(data)<br>			progress_bar.advance(task_id)<br>			<span class="hljs-keyword">return</span> result<br></code></pre></td></tr></table></figure>

<p>我们完全不用担心 <code>progress_bar</code> 对象产生争用。</p>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><p>很多功能都有现有库来实现了，所以最终代码量很小，只有 100 多行，可以实现我们开头提到的各种功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> asyncio<br><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path<br><br><span class="hljs-keyword">from</span> loguru <span class="hljs-keyword">import</span> logger<br><span class="hljs-keyword">from</span> rich <span class="hljs-keyword">import</span> progress<br><span class="hljs-keyword">from</span> langchain_openai <span class="hljs-keyword">import</span> ChatOpenAI<br><span class="hljs-keyword">from</span> langchain_core.prompts <span class="hljs-keyword">import</span> ChatPromptTemplate<br><span class="hljs-keyword">from</span> langchain_core.runnables <span class="hljs-keyword">import</span> RunnableLambda<br><span class="hljs-keyword">from</span> langchain_core.messages <span class="hljs-keyword">import</span> AIMessage<br><span class="hljs-keyword">from</span> langchain_community.cache <span class="hljs-keyword">import</span> SQLiteCache<br><br><br>progress_bar_columns = [<br>    progress.SpinnerColumn(),<br>    progress.TextColumn(<span class="hljs-string">&quot;[bold blue]&#123;task.description&#125;&quot;</span>),<br>    progress.TextColumn(<span class="hljs-string">&quot;[bold blue]&#123;task.fields[filename]&#125;&quot;</span>),<br>    progress.BarColumn(),<br>    progress.MofNCompleteColumn(),<br>    progress.TaskProgressColumn(),<br>    progress.TimeElapsedColumn(),<br>    progress.TimeRemainingColumn(),<br>]<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_chain</span>(<span class="hljs-params"></span><br><span class="hljs-params">        use_llm_cache: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        request_logprobs: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        </span>):<br>    <span class="hljs-keyword">if</span> use_llm_cache:<br>        llm_response_cache_dir = Path(<span class="hljs-string">&quot;.langchain.db&quot;</span>).as_posix()<br>        llm_cache = SQLiteCache(database_path=llm_response_cache_dir)<br>    <span class="hljs-keyword">else</span>:<br>        llm_cache = <span class="hljs-literal">False</span><br><br>    llm = ChatOpenAI(<br>        model=<span class="hljs-string">&quot;xxx&quot;</span>,<br>        api_key=<span class="hljs-string">&quot;xxx&quot;</span>,<br>        base_url=<span class="hljs-string">&quot;xxx&quot;</span>,<br>        temperature=<span class="hljs-number">0.0</span>,<br>        top_p=<span class="hljs-number">0.001</span>,<br>        max_tokens=<span class="hljs-number">500</span>,<br>        logprobs=request_logprobs,<br>        cache=llm_cache,<br>        streaming=<span class="hljs-literal">False</span>,<br>    ).bind(response_format=&#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;json_object&quot;</span>&#125;)<br><br>    prompt = Path(<span class="hljs-string">&quot;./prompt.md&quot;</span>).read_text()<br>    prompt_template = ChatPromptTemplate.from_messages([<br>        (<span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;You are a helpful assistant.&quot;</span>),<br>        (<span class="hljs-string">&quot;user&quot;</span>, prompt),<br>    ])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">find_target_token</span>(<span class="hljs-params">token_list: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">dict</span>]</span>):<br>        pattern = [<span class="hljs-string">&#x27; &quot;&#x27;</span>, <span class="hljs-string">&#x27;category&#x27;</span>, <span class="hljs-string">&#x27;&quot;:&#x27;</span>, <span class="hljs-string">&#x27; &quot;&#x27;</span>]<br>        pattern_length = <span class="hljs-built_in">len</span>(pattern)<br>        <span class="hljs-comment"># Loop through the token list with a sliding window</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(token_list) - pattern_length):<br>            current_tokens = [token_dict[<span class="hljs-string">&#x27;token&#x27;</span>] <span class="hljs-keyword">for</span> token_dict <span class="hljs-keyword">in</span> token_list[i:i + pattern_length]]<br>            <span class="hljs-keyword">if</span> current_tokens == pattern:<br>                <span class="hljs-keyword">if</span> i + pattern_length &lt; <span class="hljs-built_in">len</span>(token_list):<br>                    <span class="hljs-keyword">return</span> token_list[i + pattern_length]<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_response</span>(<span class="hljs-params">response: AIMessage</span>) -&gt; <span class="hljs-built_in">dict</span>:<br>        <span class="hljs-keyword">try</span>:<br>            res = find_target_token(response.response_metadata[<span class="hljs-string">&quot;logprobs&quot;</span>][<span class="hljs-string">&quot;content&quot;</span>])<br>            answer_logprob = <span class="hljs-built_in">float</span>(res[<span class="hljs-string">&quot;logprob&quot;</span>]) <span class="hljs-keyword">if</span> res <span class="hljs-keyword">else</span> -<span class="hljs-number">0.0</span><br>            answer_prob = math.exp(answer_logprob)<br>            parsed = json.loads(response.content)<br>            steps = <span class="hljs-string">&quot;\n&quot;</span>.join(parsed[<span class="hljs-string">&quot;reasoning_steps&quot;</span>])<br>            answer = <span class="hljs-built_in">str</span>(parsed[<span class="hljs-string">&quot;category&quot;</span>])<br>            <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;category:&quot;</span>: answer, <span class="hljs-string">&quot;prob&quot;</span>: answer_prob, <span class="hljs-string">&quot;steps&quot;</span>: steps&#125;<br>        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>            logger.warning(<span class="hljs-string">f&quot;LLM response parsing error: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br>            <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;category:&quot;</span>: <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;raw&quot;</span>: response.content&#125;<br>    <br>    <span class="hljs-keyword">return</span> prompt_template | llm | RunnableLambda(parse_response)<br><br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_task</span>(<span class="hljs-params">chain, task: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]</span>) -&gt; <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]:<br>    <span class="hljs-keyword">try</span>:<br>        result = <span class="hljs-keyword">await</span> chain.ainvoke(task)<br>        <span class="hljs-keyword">return</span> &#123;**task, **result,&#125;<br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        logger.exception(<span class="hljs-string">f&quot;Error: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br>        <span class="hljs-keyword">return</span> &#123;**task, <span class="hljs-string">&quot;error&quot;</span>: <span class="hljs-built_in">str</span>(e),&#125;<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_file_in_chunks</span>(<span class="hljs-params">filepath: <span class="hljs-built_in">str</span>, chunk_size: <span class="hljs-built_in">int</span></span>):<br>    current_batch = []<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> file:<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file:<br>            current_batch.append(line.rstrip(<span class="hljs-string">&#x27;\n&#x27;</span>))<br>            <br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(current_batch) &gt;= chunk_size:<br>                <span class="hljs-keyword">yield</span> current_batch<br>                current_batch = []<br>        <br>        <span class="hljs-comment"># Don&#x27;t forget the last partial batch</span><br>        <span class="hljs-keyword">if</span> current_batch:<br>            <span class="hljs-keyword">yield</span> current_batch<br><br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_tasks_parallel</span>(<span class="hljs-params">filepath: <span class="hljs-built_in">str</span>, max_concurrency: <span class="hljs-built_in">int</span>, chunk_size: <span class="hljs-built_in">int</span></span>):<br>    chain = build_chain()<br>    total_lines = <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">open</span>(filepath, <span class="hljs-string">&#x27;r&#x27;</span>))<br>    semaphore = asyncio.Semaphore(max_concurrency)<br>    progress_bar = progress.Progress(*progress_bar_columns)<br>    <span class="hljs-keyword">with</span> progress_bar:<br>        task_id = progress_bar.add_task(<br>            description=<span class="hljs-string">&quot;Processing&quot;</span>,<br>            total=total_lines,<br>            filename=Path(filepath).name,<br>        )<br>        <br>        <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">do</span>(<span class="hljs-params">chain, data: <span class="hljs-built_in">dict</span></span>) -&gt; <span class="hljs-built_in">dict</span>:<br>            <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> semaphore:<br>                result = <span class="hljs-keyword">await</span> run_task(data)<br>                progress_bar.advance(task_id)<br>                <span class="hljs-keyword">return</span> result<br>        <br>        <span class="hljs-comment"># --- 主循环 ---</span><br>        <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> read_file_in_chunks(filepath, chunk_size):<br>            task_dicts = [json.loads(json_text) <span class="hljs-keyword">for</span> json_text <span class="hljs-keyword">in</span> chunk]<br>            results = <span class="hljs-keyword">await</span> asyncio.gather(*[do(chain, d) <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> task_dicts])<br>            <span class="hljs-comment"># 保存结果</span><br>            ...<br><br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-keyword">await</span> process_tasks_parallel(<span class="hljs-string">&quot;./dataset.jsonl&quot;</span>, <span class="hljs-number">64</span>, <span class="hljs-number">1000</span>)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    asyncio.run(main())<br><br></code></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://openai.com/index/introducing-structured-outputs-in-the-api/">https://openai.com/index/introducing-structured-outputs-in-the-api/</a>
<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/logprobs/">https://python.langchain.com/docs/how_to/logprobs/</a>
<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/concepts/chat_models/#standard-parameters">https://python.langchain.com/docs/concepts/chat_models/#standard-parameters</a>
<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/chat_model_rate_limiting/">https://python.langchain.com/docs/how_to/chat_model_rate_limiting/</a>
<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/python/" class="print-no-link">#python</a>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>大批量调用 LLM 服务实战：以分类任务为例</div>
      <div>https://habaneraa.github.io/2024/11/20/llm-service-batch-request/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>habaneraa</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年11月20日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/11/26/software-collection/" title="我的软件收藏集">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">我的软件收藏集</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/07/02/cve-to-cwe-root-cause-mapping-guidance/" title="CVE to CWE Root Cause Mapping Guidance (双语对照)">
                        <span class="hidden-mobile">CVE to CWE Root Cause Mapping Guidance (双语对照)</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       Powered by <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> & <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
